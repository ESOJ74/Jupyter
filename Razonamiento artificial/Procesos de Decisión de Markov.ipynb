{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesos de Decisión de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los Procesos de Decisión de Markov (PDMs) encadenamos la decisiones del agente con un proceso estocástico.\n",
    "\n",
    "Nos interesa resolver la ecuación de Bellman para la política del agente.\n",
    "\n",
    "Recordemos que la política le dice al agente que acción tomar en cada estado posible.\n",
    "\n",
    "Para encontrar la politica optima $\\pi^*$ podemos usar el algoritmo de iteración de políticas.\n",
    "\n",
    "A continuación te damos una implementación básica del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from random import choice\n",
    "class MDP:\n",
    "    \n",
    "    def __init__(self,s,r,a,T,gamma):\n",
    "        \"\"\"\n",
    "        Builds the MDP problem\n",
    "        :param s: states\n",
    "        :param r: rewards\n",
    "        :param a: actions\n",
    "        :param T: dictionary where keys are (s,a) pairs and\n",
    "        values are probabilities\n",
    "        :param gamma: the discount factor\n",
    "        \"\"\"\n",
    "        self.s = s\n",
    "        self.r = r\n",
    "        self.a = a\n",
    "        self.T = T\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def policy_iteration(self,pi=None):\n",
    "        \"\"\"\n",
    "            Policy iteration algorithm\n",
    "        \"\"\"\n",
    "        #initial random policy\n",
    "        if not pi:\n",
    "            pi = [choice(self.a) for s in self.s]\n",
    "        \n",
    "        print('pi = '+str(pi))\n",
    "        self.obtainT(pi);\n",
    "        T = self.obtainT(pi)\n",
    "        print('T(pi) = \\n'+str(T))\n",
    "        \n",
    "        V = np.matmul(np.linalg.inv(np.eye(3)-self.gamma*T.T),self.r)\n",
    "        print('V(s) = '+ str(V))\n",
    "        \n",
    "        pi_star = self.find_pi_star(V)\n",
    "        print(\"pi*(s) = \"+str(pi_star))\n",
    "        \n",
    "        if pi_star == pi:\n",
    "            return pi\n",
    "        else:\n",
    "            return self.policy_iteration(pi_star)\n",
    "        \n",
    "    def obtainT(self,pi):\n",
    "        \"\"\"\n",
    "        Obtains the transition probability matrix parametrized by the policy pi\n",
    "        :param pi: the policy\n",
    "        \"\"\"\n",
    "        return \\\n",
    "          np.matrix([[self.T[(s,t,pi[s])] for s in self.s] for t in self.s])\n",
    "        \n",
    "    def find_pi_star(self,V):\n",
    "        \"\"\"\n",
    "        Finds the optimal policy for the given infinite horizon values\n",
    "        :param V: the infinite horizon expected utility\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\".join([str((x,np.matmul(self.obtainT(x).T,np.array(V).T))) for x in product(self.a,self.a,self.a)]))\n",
    "        \n",
    "        return list(max(product(self.a,self.a,self.a),\\\n",
    "            key=lambda x: np.sum(np.matmul(self.obtainT(x).T,np.array(V).T))))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar el algoritmo primero definimos los estados posibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "estados = [0,1,2]\n",
    "print(estados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las acciones posibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "acciones = [0,1]\n",
    "print(acciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora las recompensas para cada estado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 27]\n"
     ]
    }
   ],
   "source": [
    "recompensas = [0,10,27]\n",
    "print(recompensas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro gamma es el factor de descuento para la utilidad de valores futuros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora representamos las probabilidaddes condicionales con un diccionario.\n",
    "\n",
    "<img src=\"mdpejemplo2.png\" alt=\"mdp\" width=\"914\"/>\n",
    "\n",
    "La llave será una tupla con tres elementos, los primeros dos contienen los índices de la transición de estado, el último es la acción que se toma.\n",
    "\n",
    "El valor del diccionario es la probabilidad asociada al estado, acción correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 0): 0.7, (0, 0, 1): 0.5, (1, 0, 0): 0.4, (1, 0, 1): 0.2, (2, 0, 0): 0.2, (2, 0, 1): 0.1, (0, 1, 0): 0.1, (0, 1, 1): 0.3, (1, 1, 0): 0.4, (1, 1, 1): 0.7, (2, 1, 0): 0.2, (2, 1, 1): 0.1, (0, 2, 0): 0.2, (0, 2, 1): 0.2, (1, 2, 0): 0.2, (1, 2, 1): 0.1, (2, 2, 0): 0.6, (2, 2, 1): 0.8}\n"
     ]
    }
   ],
   "source": [
    "T={\n",
    "    (0,0,0):0.7,(0,0,1):0.5, (1,0,0):0.4,(1,0,1):0.2, (2,0,0):0.2,(2,0,1):0.1,\n",
    "    (0,1,0):0.1,(0,1,1):0.3, (1,1,0):0.4,(1,1,1):0.7, (2,1,0):0.2,(2,1,1):0.1,\n",
    "    (0,2,0):0.2,(0,2,1):0.2, (1,2,0):0.2,(1,2,1):0.1, (2,2,0):0.6,(2,2,1):0.8\n",
    "}\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una instancia del PDM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(estados,recompensas,acciones,T,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que obtenemos la misma política que el ejemplo del tren inteligente.\n",
    "\n",
    "En el ejemplo $\\pi_0 = \\begin{bmatrix}0&0&0\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(pi_0) = \n",
      "[[0.7 0.4 0.2]\n",
      " [0.1 0.4 0.2]\n",
      " [0.2 0.2 0.6]]\n"
     ]
    }
   ],
   "source": [
    "pi_0 = [0,0,0]\n",
    "print(\"T(pi_0) = \\n\"+str(mdp.obtainT(pi_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora invocamos el algoritmo de iteración de políticas.\n",
    "\n",
    "El algoritmo imprime como cambia la política con las iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi = [0, 0, 0]\n",
      "T(pi) = \n",
      "[[0.7 0.4 0.2]\n",
      " [0.1 0.4 0.2]\n",
      " [0.2 0.2 0.6]]\n",
      "V(s) = [[ 91.73373288 105.43236301 135.84760274]]\n",
      "((0, 0, 0), matrix([[101.92636986],\n",
      "        [106.0359589 ],\n",
      "        [120.94178082]]))\n",
      "((0, 0, 1), matrix([[101.92636986],\n",
      "        [106.0359589 ],\n",
      "        [128.39469178]]))\n",
      "((0, 1, 0), matrix([[101.92636986],\n",
      "        [105.73416096],\n",
      "        [120.94178082]]))\n",
      "((0, 1, 1), matrix([[101.92636986],\n",
      "        [105.73416096],\n",
      "        [128.39469178]]))\n",
      "((1, 0, 0), matrix([[104.66609589],\n",
      "        [106.0359589 ],\n",
      "        [120.94178082]]))\n",
      "((1, 0, 1), matrix([[104.66609589],\n",
      "        [106.0359589 ],\n",
      "        [128.39469178]]))\n",
      "((1, 1, 0), matrix([[104.66609589],\n",
      "        [105.73416096],\n",
      "        [120.94178082]]))\n",
      "((1, 1, 1), matrix([[104.66609589],\n",
      "        [105.73416096],\n",
      "        [128.39469178]]))\n",
      "pi*(s) = [1, 0, 1]\n",
      "pi = [1, 0, 1]\n",
      "T(pi) = \n",
      "[[0.5 0.4 0.1]\n",
      " [0.3 0.4 0.1]\n",
      " [0.2 0.2 0.8]]\n",
      "V(s) = [[127.58241758 138.57142857 181.97802198]]\n",
      "((0, 0, 0), matrix([[139.56043956],\n",
      "        [142.85714286],\n",
      "        [162.41758242]]))\n",
      "((0, 0, 1), matrix([[139.56043956],\n",
      "        [142.85714286],\n",
      "        [172.1978022 ]]))\n",
      "((0, 1, 0), matrix([[139.56043956],\n",
      "        [140.71428571],\n",
      "        [162.41758242]]))\n",
      "((0, 1, 1), matrix([[139.56043956],\n",
      "        [140.71428571],\n",
      "        [172.1978022 ]]))\n",
      "((1, 0, 0), matrix([[141.75824176],\n",
      "        [142.85714286],\n",
      "        [162.41758242]]))\n",
      "((1, 0, 1), matrix([[141.75824176],\n",
      "        [142.85714286],\n",
      "        [172.1978022 ]]))\n",
      "((1, 1, 0), matrix([[141.75824176],\n",
      "        [140.71428571],\n",
      "        [162.41758242]]))\n",
      "((1, 1, 1), matrix([[141.75824176],\n",
      "        [140.71428571],\n",
      "        [172.1978022 ]]))\n",
      "pi*(s) = [1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "politica = mdp.policy_iteration(pi_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si cambiamos el valor las recompensas a  [0,20,20] , ¿cambia la politica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cambia las recompensas y ejecuta la siguiente línea. \n",
    "#politica = mdp.policy_iteration([0,1,0])\n",
    "\n",
    "#copia el resultado en un archivo y sométe la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
